In this hands-on session, we’ll build a real-time analytics pipeline using live flight data and modern streaming tools — step by step, and without hand-waving.
We’ll start by streaming data into Apache Kafka using Confluent Cloud, process it with Apache Flink, and land it in open table formats using Tableflow. From there, we’ll query the data with Trino and turn it into dashboards using Superset. Everything runs in a practical setup you can reproduce later, including Docker and cloud-managed services.
Along the way, we’ll also look at anomaly detection and simple predictions using built-in functions, so you can see how real-time insights can be added without building custom ML pipelines or complex models.
This session is all about doing, not slides. You’ll see how these pieces actually fit together, what decisions matter in practice (regions, credentials, formats), and how to go from streaming data to analytics in a way that scales.
If you’re curious about real-time analytics, Kafka + Flink in the cloud, or how streaming, analytics, and lightweight predictive use cases come together in the same pipeline, this workshop will give you a clear, working mental model — and code you can take home.
